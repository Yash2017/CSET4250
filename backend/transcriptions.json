{
  "1": "It's just like the third week. OK, we're going to go ahead and get started. Last week's homework assignment has been extended to be due tomorrow at midnight, because I forgot we only had one class last week, so some of the material on the second page we're covering today, so it didn't make much sense to have it due before we covered it. Today we're talking about chapter three. Chapter three talks about syntax and semantics. Syntax are the rules of our statements, of our language that we have to follow. And semantics are the meaning of those statements. So syntax are the rules that we have to follow to when we're creating our programs for our commands and statements. And the semantics are the meaning of those statements, commands and statements that we're trying to perform. Put both of those together, and that's what makes up a computer programming language. A sentence is a set of characters over some alphabet. And a language is a set of sentences. So a computer programming language is a set of controlled sentences. A lexime is the lowest entity, lowest form of entity that we have in our program. So everything, and you'll see next week when we go through exactly how a compiler works, and we break it down. The very first thing a compiler has to do is break everything down to its smallest unit. So we break it down into what we call leximes. And a token is just a group or a category of leximes. In a language, we have recognizers and generators. Generators are the things that create the, in our case, the programs themselves. Recognizers are the things that make sure that we follow the rules of the language. So a generator could be a computer programmer. And a recognizer is used either at execution time or at compile time, if it's a compiled language, to verify that we follow the rules. We follow the syntax of our commands and statements. You'll see next week when we go over how a compiler works, we have a syntax analyzer that breaks everything down, verifies that we follow the rules. If we don't follow the rules, we get a syntax error, and we cease execution. And our program no longer gets CPU time. So you don't remember anything else about this particular chapter today. Remember the two things we just talked about and the context-free grammar that we use to display our syntax, our rules, is BNF. We use BNF and eBNF. We'll talk about the difference in a few minutes. But today, pretty much all languages use BNF or eBNF to display the syntax of our commands and statements. It's a context-free grammar. What does that mean? We use symbols to display things because we humans can recognize and interpret symbols much faster than reading through and have to stop and think. We can generally determine the syntax of a statement before we even blink just by looking at it because we're looking at the symbols, and the symbols mean something to us. So in BNF, we have what's called terminals and non-terminals. Terminals are just the leximes themselves or the group of leximes, the tokens. We have a right-hand side and a left-hand side. The left-hand side is generally going to have only non-terminals. Non-terminals are the statement itself that we're trying to define the rules for. And then the right-hand side can have a mixture of non-terminals and terminals. We can't have more than one right-hand side. We can have more than one set of rules that we have to follow. The right-hand side is the set of rules that we're going to follow for whatever the command or statement is on the left-hand side. So on this example, we have our left-hand side, which is a non-terminal. We have our right-hand side, telling us the rules we're going to follow for that. And then we have our start symbol. Now, these days, originally, the right arrow was used as the start symbol. These days, some languages change that. Some put a colon in there. Some of them, because the colon isn't picked up very well on a printer, some of them put a colon, equal sign, or a colon, and then the right arrow sign. And then if your Microsoft's in there for a reason, they just leave it blank. They don't put a colon. So the start symbol is the separator from our left-hand side, or what we're trying to describe the rules for. And the right-hand side are the rules we're going to follow. So the rules we have to follow for that command or statement. And we can have more than one right-hand side. We can have more than one set of rules we have to follow. BNF or EVNF is used by both the generator, the programmer creating the program, and the compiler verifying that we follow the rules. So we're using the exact same thing. What the programmer is using to determine what the rules are is what the compiler is using to see if we follow the rules. So we're not trying to duplicate anything or recreate something. We're using the exact same BNF or EVNF. Now, we're going to talk a lot about parsing trees next week. What we're trying to do, or what the compiler is trying to do to verify we follow the rules is create a parsing tree. A parsing tree is, first, we're going to separate all these things in the left-hand side, and we're going to put the left-hand side into a tree to follow them so that we can determine whether or not we follow the rules. The tree is what tells us what the rules are. It's a hierarchical representation of our statements, commands or statements. If it's ambiguous, if our statement is ambiguous, that means we have more than one parsing tree. If we only have one parsing tree, it's unambiguous. Next week, when we go through our compiler works, we only have one parsing tree. It's going to be much easier to determine whether we follow the rules or not. If it's ambiguous and we have more than one parsing tree for the same statement, the compiler has to determine which one of those sets of rules, which one of those trees you think you're trying to follow. So it has to determine or make an attempt to determine which one of those sets of rules, what's one of those right-hand sides you're trying to follow, and then determine whether or not you follow those rules or not. So if it's ambiguous, we have more than one parsing tree, it's going to be more difficult to determine whether or not you follow the rules, because we have to first figure out which set of rules you think you're trying to follow. If it's unambiguous, we have one parsing tree. What are associativity rules and precedent rules of a language? What does that mean? What's precedent? What's that? What happens first. What happens first, which was, which items we're going to process first. And in our operations, it can make a big difference, right? We have to know the order of precedence of the language. What's the associativity rules? Here's an example of associativity rules of a parsing tree for associativity rules. What is associativity rule? What's the difference between precedence rules and associativity rules? Is it important? Only if you want to get the right answer, right? Associativity rules are when there's a tie. In this case, we have addition. We have two plus signs that are equal precedence, right? So which one are we going to process first? If I don't process them in the right order, in the correct order, I'm going to get a different answer. So our parsing tree has to determine the precedence rules and associativity rules when it lays out the parsing tree. Now we also have this thing called EBNF, extended BNF. Extended BNF is actually what most languages actually follow today. Extended BNF added some symbols. The brackets, the braces, the parentheses with things like plus, minus, and the braces down here. It added those symbols so that we could explain the same rules but in less real estate, in a smaller area. So what the extended BNF does is allow us to say the same thing as BNF would say, but with fewer lines. The fewer lines we have as human beings, when we get to understand what the symbols are, the quicker we can determine what the actual rules are. We have more lines to look at. This is an example of BNF. And you can do the same thing in these two lines using EBNF. And I could more quickly determine what the full functionality of the rules are for this statement that happened to read through multiple lines. So we said for our start separator, many languages today have different symbols for the start symbol. And Microsoft decided not to even have one. They just used the one for the start symbol. So we're going to use the one for the start symbol. And we're going to use the one for the end symbol. They just put two blanks there. So there were a lot of just empty spot in between. And you'll see when you do this next homework assignment how Microsoft does theirs. But it's generally, because originally there is an arrow. An arrow was an ASCII code. It was hard to get the correct. They went to the equal sign with the greater than sign to make it look like an arrow. And then they went with the colon with all three of them. And then some of them have just decided not to even use it as a separator. What is, when you're computer programming, what does the word static mean? What's the difference between the word static and dynamic in computer programming? Static states the same. Static states the same. Dynamic can change throughout the execution of the program. What does that mean for us programmers? This is something that you're going to want to remember throughout the whole semester. The earlier you remember this part, the easier some of these programming things are. When we start talking about static and dynamic. If it stays the same, what does that tell me? If it's static, it's going to stay the same. I can determine it at compile time. Before I even execute the program, it can be determined. It's static. It's going to stay the same. If it's dynamic, I can only determine it during execution time. So if it's static, it means it can be determined before we even load the program up. It's actually determined at load time for purposes of this course. Before we actually begin executing the program. It's already set and installed. If it's dynamic, it's going to change throughout the execution of the program. I can only determine it when the program is running. So if it's static semantics, what does that tell us? That the meaning is going to be consistent. Static. It's going to stay the same. Remember we said semantics? It tells us what? The meaning of our commands and statements. Now when we talk about the syntax of our commands and statements, we said we are following BNF or EBNF to describe them. And pretty much all languages today support that mechanism. We don't have any universal mechanism to describe the semantics of our commands and statements like we do with the syntax. So, and this is going to come into play a lot next week when we talk about because the problem, the biggest problem with a compiler is trying to figure out which one of those right-hand sides we're using. The way you're trying to figure out which one of those right-hand sides we're using is to determine the meaning of our statements. And you're going to see we have that parsing tree We have multiple ways to chase down the parsing tree to determine whether or not we follow the rules. We can start at the top and work our way down, or we can start at the bottom and work our way up. So if we start at the bottom, we're starting at the leaves and working our way back up to the root. We start at the top, we're working up the root, we're working our way down to the leaves. And then we'll talk about which one's more accurate. Next week we'll get into actually dissecting how a compiler works and what we do to verify that we follow the rules. Now to truly be able to determine the semantics of our statements we would have to run them through and then work our way back That would take a virtual machine and compile time to do that. You would have to actually process the statements, see what the results were, and work your way back. That's not practical in a compiler. That's the only way to truly 100% determine the semantics of our statements or logarithms or operations. That would take way more overhead than what we want. Our cost to perform those and we said a peer interpreter is going to be processing those instructions and determining whether we follow the rules or not one line at a time. It doesn't know what the next line is until it has processed the line that's on. So a peer interpreter has more issues breaking down those leximes A compiler ahead of time is going to be able to break down all the leximes put them in the categories and build that parsing tree before it even starts determining what order to process things. A peer interpreter, remember, is reading things in one area at a time grabbing those leximes checking to verify that they're valid that they're either part of the language itself or they've been created allocated, they exist by our program or book otherwise we're referencing something that doesn't exist so that's why our interpreter languages are by far more unreliable as far as one-time errors go because they can't go through and validate that we follow the rules ahead of time they don't have the information to break everything into leximes before they even start like our compiler does what is recursion? we're going to talk a lot about recursion in this class what is recursion? we talked a little bit about it last class you see the term a lot in this book you're going to see the term a lot in programming in general what does it mean? ok, so something is called a sub-procedure or a function that is called more than once in the same program that's part of it what does it mean if something supports recursion? because I can call a sub-procedure a function multiple times and not support recursion the value the starting values are going to be the same each time remember we talked about that last class we created local variables so we want to support recursion but in this class we're going to talk about we absolutely positively want to support recursion because if we support recursion what does that do for us? if I call a sub-procedure and I'm using public variables global variables and I'm allowing that sub-procedure to make changes to those global variables I've just given up recursion and if I give up recursion what have I given up? that's the most important thing I can ask in the program process abstraction because now every time I call that sub-procedure I have to go through every line of that sub-procedure and see if it's making changes to variables that are in the caller program it's no longer abstracting I've just given up the thing that I want to hold just that's the thing that's going to allow me to make my changes quickly without having to worry about what's in that sub-program if I need to make changes to my global variables, if I need to make changes to the values of my global variables I can call a sub-program and have that sub-program return a value back to my caller program using a parameter or if it's a function and then I can change that value in the caller program I call a sub-program, when it comes back I tell it to change the value that was returned to me so it's right there in front of me in the caller program I've kept my recursion intact which means I've kept my process abstraction intact, which means I can ignore what's in that program because anytime I'm making changes it's going to be right in the caller program so I get the same result the value gets changed, it's just going to be right in front of me in the caller program I no longer have to go look at that sub-procedure when I call it to see what it's doing or if I'm making changes to the sub-procedure I don't want to have to go look at every program that that sub-procedure is being called from when I build my sub-procedures I want to build them generic enough that I can reuse them in other programs in the future, I want to build a library, I don't want to have to reinvent the wheel I have a problem to solve I'm going to solve that problem make it generic enough to where I can use it again in the future put it in a library that I can link in at compile time now I make changes to that sub-procedure if it's in a hundred different programs I don't want to have to go through every one of those hundred different programs to see if it's going to cause them problems, I want those hundred programs to be abstracted I can just focus on the sub-procedure make changes in there knowing that it's not going to negatively impact anything in those hundred programs that are calling process abstraction is one of the most important things we as a programmer have to help us with that long term life cycle that we have in our programs it's going to be in maintenance mode and production mode much longer than it's going to be in development mode okay if I do something that saves me time in development mode but it's going to cost me a lot of time and aggravation in production mode that's not a good thing I want to be able to pass this program off to a maintenance group and move on and do something new I don't want to be stuck being the one that has to maintain this program forever I want to make it easy to maintain it the way we make things easy to maintain is process abstraction and recursion is one of the keys to process abstraction just because a language permits us to do something doesn't mean we want to do it you generally never ever ever want to change a value from a caller program from our subprogram you can generally when we go through and see how these things work you can generally tell a mile away someone who doesn't understand scope if I'm using all global variables if I'm using all global variables and I'm changing the values of those global variables in my subprocedures yeah I can get it to work but is it going to be manageable and be able to be out in the field definitely isn't going to be able to be maintained by someone else that's what got us into the problem in the 1960s after the 1950s people just did whatever they had to do to get their programs to work so we want to be able to have recursion in our subprocedure calls to have recursion in our subprocedure calls we have to use local variables to the subprogram and don't change any of the values from the caller program the very second we change the value in the caller program we've given up process of protection if we create global variables in our subprocedure and we set those values and we leave if they're global that means they are static right when we're talking about that difference between static and dynamic our global variables are static and our local variables are dynamic generally when we're talking about static and dynamic we're talking about the heat versus the stat what's the difference between the two stack is what stack is the first and the first time last time heat is the heat is our static variables that we put on the heat now we do have some languages that have dynamic heat and we'll go through those we're going to go through all this in detail throughout the semester and then the stack is our dynamic our local variables it's like a stack of plates we put our new variables on top of the stack and we can only take off from the top of the stack so we add to the top and we move from the top so you create your variables in your subprogram those variables those local variables are put on top of the stack we process our instructions in our subprogram when we return control back to the caller program those local variables in our subprogram are removed from the top of the stack so the stack is for dynamic addressing and the heat is for the stack so when you talk about global variables you're usually talking about the heat so if we create global variables in our subprogram and our subprogram returns control back to the main caller program what have we done now we call that now we call that subprogram again those global variables whatever the value was left last time is still there not starting over so whatever values I stored in those memory addresses are going to be there I call it ten times it could be a different value every time I call it I no longer have recursion that's what we mean by recursion the easiest way to support recursion in our subprocedure is to use local variables those local variables are put on top of the stack when the subprogram is done and returns control back to the caller program they're removed from the stack so the next time they're called they're going to be put back on the stack with those beginning values again will be the same every time we do not have the homework assignment to review today so that's where I want to stop so we'll go over more on Wednesday so remember the homework assignment is due tomorrow at midnight you should have at least been able to do everything on the first page so there's just a few questions on the back page to do and then we'll review that homework assignment finish up this chapter and start talking about how the compiler works for next week there will still be a homework assignment a little programming type of assignment this week that we'll go over on Wednesday as well any questions about anything we talked about today any questions about the assignment if you've turned in the assignment and you want to make changes and resubmit before midnight you can resubmit those as many times as you want so you can just add the second sheet answers and modify them and resubmit and then I just grade the last one if you haven't signed in make sure you sign in otherwise have a good day",
  "2": "It's just like the third week. OK, we're going to go ahead and get started. Last week's homework assignment has been extended to be due tomorrow at midnight, because I forgot we only had one class last week. So some of the material on the second page we're covering today, so it didn't make much sense to have it due before we covered it. Today we're talking about chapter three. Chapter three talks about syntax and semantics. Syntax are the rules of our statements, of our language that we have to follow. And semantics are the meaning of those statements. So syntax are the rules that we have to follow to when we're creating our programs for our commands and statements. And the semantics are the meaning of those statements, commands and statements that we're trying to perform. Put both of those together, and that's what makes up a computer programming language. A sentence is a set of characters over some alphabet. And a language is a set of sentences. So a computer programming language is a set of controlled sentences. A lexime is the lowest entity, lowest form of entity that we have in our program. So everything, and you'll see next week when we go through exactly how a compiler works, and we break it down. The very first thing a compiler has to do is break everything down to its smallest unit. So we break it down into what we call leximes. And a token is just a group or a category of leximes. In a language, we have recognizers and generators. Generators are the things that create the, in our case, the programs themselves. Recognizers are the things that make sure that we follow the rules of the language. So a generator could be a computer programmer. And a recognizer is used either at execution time or at compile time, if it's a compiled language, to verify that we follow the rules. We follow the syntax of our commands and statements. You'll see next week when we go over how a compiler works, we have a syntax analyzer that breaks everything down, verifies that we follow the rules. If we don't follow the rules, we get a syntax error, and we cease execution. And our program no longer gets CPU time. So you don't remember anything else about this particular chapter today. Remember the two things we just talked about and the context-free grammar that we use to display our syntax, our rules, is BNF. We use BNF and eBNF. We'll talk about the difference in a few minutes. But today, pretty much all languages use BNF or eBNF to display the syntax of our commands and statements. It's a context-free grammar. What does that mean? We use symbols to display things because we humans can recognize and interpret symbols much faster than reading through and have to stop and think. We can generally determine the syntax of a statement before we even blink just by looking at it because we're looking at the symbols, and the symbols mean something to us. So in BNF, we have what's called terminals and non-terminals. Terminals are just the leximes themselves or the group of leximes, the tokens. We have a right-hand side and a left-hand side. The left-hand side is generally going to have only non-terminals. Non-terminals are the statement itself that we're trying to define the rules for. And then the right-hand side can have a mixture of non-terminals and terminals. We can't have more than one right-hand side. We can have more than one set of rules that we have to follow. The right-hand side is the set of rules that we're going to follow for whatever the command or statement is on the left-hand side. So on this example, we have our left-hand side, which is a non-terminal. We have our right-hand side, telling us the rules we're going to follow for that. And then we have our start symbol. Now, these days, originally, the right arrow was used as the start symbol. These days, some languages change that. Some put a colon in there. Some of them, because the colon isn't picked up very well on a printer, some of them put a colon, equal sign, or a colon, and then the right arrow sign. And then if your Microsoft's in there for a reason, they just leave it blank. They don't put a colon. So the start symbol is the separator from our left-hand side, or what we're trying to describe the rules for. And the right-hand side are the rules we're going to follow. So the rules we have to follow for that command or statement. And we can have more than one right-hand side. We can have more than one set of rules we have to follow. BNF or EVNF is used by both the generator, the programmer creating the program, and the compiler verifying that we follow the rules. So we're using the exact same thing. What the programmer is using to determine what the rules are is what the compiler is using to see if we follow the rules. So we're not trying to duplicate anything or recreate something. We're using the exact same BNF or EVNF. Now, we're going to talk a lot about parsing trees next week. What we're trying to do, or what the compiler is trying to do to verify we follow the rules, is create a parsing tree. A parsing tree is, first, we're going to separate all these things in the left-hand side, and then we're going to put the left-hand side into a tree so that we can determine whether or not we follow the rules. The tree is what tells us what the rules are. It's a hierarchical representation of our statements, commands or statements. If it's ambiguous, if our statement is ambiguous, that means we have more than one parsing tree. If we only have one parsing tree, it's unambiguous. Next week, when we go through our compiler works, we only have one parsing tree. It's going to be much easier to determine whether we follow the rules or not. If it's ambiguous and we have more than one parsing tree for the same statement, the compiler has to determine which one of those sets of rules, which one of those trees you think you're trying to follow. So it has to determine or make an attempt to determine which one of those sets of rules, what's one of those right-hand sides you're trying to follow, and then determine whether or not you follow those rules or not. So if it's ambiguous, we have more than one parsing tree, it's going to be more difficult to determine whether or not you follow the rules, because we have to first figure out which set of rules you think you're trying to follow. If it's unambiguous, we have one parsing tree. What are associativity rules and precedent rules of a language? What does that mean? What's precedent? What's that? What happens first. What happens first, which was, which items we're going to process first. And in our operations, it can make a big difference, right? We have to know the order of precedence of the language. What's the associativity rules? So here's an example of associativity rules of a parsing tree for associativity rules. What is the associativity rule? What's the difference between precedence rules and associativity rules? Is it important? Only if you want to get the right answer, right? Associativity rules are when there's a tie. In this case, we have addition. We have two plus signs that are equal precedence, right? So which one are we going to process first? If I don't process them in the right order, in the correct order, I'm going to get a different answer. So our parsing tree has to determine the precedence rules and associativity rules when it lays out the parsing tree. Now, we also have this thing called EBNF, extended BNF. Extended BNF is actually what most languages actually follow today. Extended BNF added some symbols. The brackets, the braces, the parentheses with things like plus, minus, and the braces down here. It added those symbols so that we could explain the same rules, but in less real estate, in a smaller area. So what the extended BNF does is allow us to say the same thing as BNF would say, but with fewer lines. The fewer lines we have as human beings, when we get to understand what the symbols are, the quicker we can determine what the actual rules are. We have more lines to look at. We have more lines to look at. This is an example of BNF, and you can do the same thing in these two lines using EBNF. And I could more quickly determine what the full functionality of the rules are for this statement that happened to read through multiple lines. So we said for our start separator, many languages today have different symbols for the start symbol. And Microsoft decided not to even have one. They just put two blanks there, so there are a large empty spot in between. And you'll see when you do this next homework assignment how Microsoft does theirs. But it's generally, because originally there is an arrow. An arrow was an ASCII code. It was hard to get to print. So they went to the equal sign with the greater than sign to make it look like an arrow. And then they went with the colon with all three of them. And then some of them have just decided not to even use it as a separator. What is, when you're computer programming, what does the word static mean? What's the difference between the word static and dynamic in computer programming? Static states the same, dynamic can change. OK, static states the same, dynamic can change throughout the execution of the program. What does that mean for us programmers? This is something that you're going to want to remember throughout the whole semester. The earlier you remember this part, the easier some of these programming things are. When we start talking about static and dynamic, if it stays the same, what does that tell me? If it's static, it's going to stay the same. I can determine it at compile time. Before I even execute the program, it can be determined. It's static, it's going to stay the same. If it's dynamic, I can only determine it during execution time. OK, so if it's static, it means it could be determined before we even load the program up. It's actually determined at load time for purposes of this course, before we actually begin executing the program. It's already set and installed. If it's dynamic, it's going to change throughout the execution of the program. I can only determine it when the program is running. So if it's static semantics, what does that tell us? It tells us that the meaning is going to be consistent. Static, right, it's going to stay the same. Remember, we said semantics means what? It tells us what? The meaning of our commands and statements, right? Now, when we talk about the syntax, of our commands and statements, we said we are following BNF or EBNF to describe them. And pretty much all languages today support that mechanism. We don't have any universal mechanism to describe the semantics of our statements, commands and statements, like we do with the syntax. So, and this is going to come into play a lot next week when we talk about how a compiler works. Because the problem, the biggest problem with a compiler is trying to figure out which one of those right-hand sides we're using. The way you're trying to figure out which one of those right-hand sides we're using is to determine the meaning of our statements. And you're going to see we have that parsing tree. We have multiple ways to chase down the parsing tree to determine whether or not we follow the rules. We can start at the top and work our way down, or we can start at the bottom and work our way up. So if we start at the bottom, we're starting at the leaves and working our way back up to the root. If we start at the top, we're working up the root, working our way down to the leaves. And then we'll talk about which one's more accurate. Next week, we'll get into actually dissecting how a compiler works and what we do to verify that we follow the rules. Now, to truly be able to determine the semantics of our statements, we would have to run them through and then work our way back. That would take a virtual machine at compile time to do that. You would have to actually process the statement, see what the statement results were, and work your way back. That's not practical in a compiler. That's the only way to truly 100% determine the semantics of our statements or our logarithms or operations. That would take way more overhead than what we want. Our cost and resources that we talked about to perform those. And we said a peer interpreter is going to be processing those instructions and determining whether we follow the rules or not one line at a time. It doesn't know what the next line is until it has processed the line it's on. So a peer interpreter has more issues breaking down those leximes. A compiler, ahead of time, is going to be able to break down all the leximes, put them into categories, and build that parsing tree before it even starts determining what order to process things. A peer interpreter, remember, is reading things in one area at a time, grabbing those leximes, checking to verify that they're valid, that they're either part of the language itself, or they've been created, allocated, they exist by our program or book. Otherwise, we're referencing something that doesn't exist. So that's why our interpreter languages are by far more unreliable as far as one-time errors go, because they can't go through and validate that we follow the rules ahead of time. They don't have the information to break everything into leximes before they even start like our compiler does. What is recursion? We're going to talk a lot about recursion in this class over the semester. What is recursion? We talked a little bit about it last class. You see the term a lot in this book. You're going to see the term a lot in programming in general. What does it mean? So something is called a sub-procedure or a function that is called more than once in the same program. That's part of it. What does it mean if something supports recursion? Because I can call a sub-procedure or a function multiple times and not support recursion with it. The value, the starting values are going to be the same each time. Remember, we talked about that last class. We created local variables. So we want to support recursion. But in this class, we're going to talk about, we absolutely positively want to support recursion. Because if we support recursion, what does that do for us? If I call a sub-procedure and I'm using public variables, the global variables, and I'm allowing that sub-procedure to make changes to those global variables, I've just given up recursion. And if I give up recursion, what have I given up? That's the most important thing I can have in the program. Process abstraction. Because now, every time I call that sub-procedure, I have to go through every line of that sub-procedure and see if it's making changes to variables that are in the caller program. It's no longer abstracting. I've just given up the thing that I want to hold dearest. Because that's the thing that's going to allow me to make my changes quickly without having to worry about what's in that sub-program. If I need to make changes to my global variables, if I need to make changes to the values of my global variables, I can call a sub-program and have that sub-program return a value back to my caller program using a parameter, or if it's a function, using the value of the function. And then I can change that value in the caller program. I call a sub-program. When it comes back, I tell it to change the value that was returned to me. So it's right there in front of me in the caller program. I've kept my recursion intact, which means I've kept my process abstraction intact, which means I can ignore what's in that program. Because any time I'm making changes, it's going to be right in the caller program. So I get the same result. The value gets changed. It's just it's going to be right in front of me in the caller program. Or if I'm making changes to the sub-procedure, I don't want to have to go look at every program that that sub-procedure is being called from. When I build my sub-procedures, I want to build them generic enough that I can reuse them in other programs. So I can build my sub-procedures in a generic way. I can build my sub-procedures in a generic way that I can reuse them in other programs in the future. I want to build a library. I don't want to have to reinvent the wheel. I have a problem to solve. I'm going to solve that problem, make it generic enough to where I can use it again in the future, put it in a library that I can link in at compile time. Now I make changes to that sub-procedure. If it's in 100 different programs, I don't want to have to go through every one of those 100 different programs to see if it's going to cause them problems. I want those 100 programs to be abstracted. I can just focus on the sub-procedure, make changes in there, knowing that it's not going to negatively impact anything in those 100 programs that are calling. So process abstraction is one of the most important things we as a programmer have to help us with that long-term life cycle that we have in our programs. It's going to be in maintenance mode and production mode much longer than it's going to be in development mode. If I do something that saves me time in development mode, but it's going to cost me a lot of time and aggravation in production mode, that's not a good thing. I want to be able to pass this program off to a maintenance group and move on and do something new. I don't want to be stuck being the one that has to maintain this program forever. I want to make it easy to maintain it. The way we make things easy to maintain is process abstraction. And recursion is one of the keys to process abstraction. Just because a language permits us to do something doesn't mean we want to do it. We generally never, ever, ever want to change a value from a color program from our subprogram. You can generally, when we go through and see how these things work, you can generally tell a mile away someone who doesn't understand scope that they use all global variables. If I'm using all global variables and I'm changing the values of those global variables in my subprocedures, yeah, I can get it to work. But is it going to be manageable and be able to be out in the field, be maintained? Definitely, is it going to be maintained by someone else? That's what got us into the problem in the 1960s. After the 1950s, people just did whatever they had to do to get their programs to work. So we want to be able to have recursion in our subprocedure calls. To have recursion in our subprocedure calls, we have to use local variables to the subprogram and don't change any of the values from the color program. The very second we change a value in the color program, we've given up process abstraction. If we create global variables in our subprocedure and we set those values and we leave them, if they're global, that means they are static, right? So when we're talking about that difference between static and dynamic, our global variables are static and our local variables are dynamic. Generally, when we're talking about static and dynamic, we're talking about the heat versus the stack. What's the difference between the two? Stack is what? Stack is what? Stack is the first in the first out. Last in the first out. Last in the first out. The heap is our static variables. I generally put on the heap. Now, we do have some languages that say they have dynamic heap, and we'll go through those. We're going to go through all this in detail throughout the semester. And then the stack is our dynamic, our local variables we. It's like a stack of plates. We put our new variables on top of the stack, and we can only take off from the top of the stack. So we add to the top, and we move from the top. So you create your variables in your subprogram. Those variables, those local variables are put on top of the stack. We process our instructions in our subprogram. When we return control back to the caller program, those local variables in our subprogram are removed from the top of the stack. So the stack is for dynamic addressing, and the heap is for the stack. So when you talk about global variables, you're usually talking about the heap. So if we create global variables in our subprogram, and our subprogram returns control back to the main caller program, what have we done? Now we call that subprogram again. Those global variables, whenever the value was left last time, it's still there, not starting over. So whatever values I stored in there with memory addresses are going to be there. I call it 10 times, it could be a different value every time I call it. I no longer have recursion. That's what we mean by recursion. The easiest way to support recursion in our subprocedure is to use local variables. Those local variables are put on top of the stack. When the subprogram is done and returns control back to the caller program, they're removed from the stack. So the next time they're called, they're going to be put back on the stack with those beginning values again. They'll be the same every time. We do not have the homework assignment to review today. This is where I want to stop, so we'll go over more on Wednesday. So remember, the homework assignment is due tomorrow at midnight. You should have at least been able to do everything on the first page, so there's just a few questions on the back page to do. And then we'll review that homework assignment, finish up this chapter, and start talking about how a compiler works for next year, and give a homework. There will still be a homework assignment, a little programming type of assignment this week that we'll go over on Wednesday as well. Any questions about anything we talked about today? Any questions about the assignment? If you've turned in the assignment and you want to make changes and resubmit before midnight, you can resubmit those as many times as you want. So you can just add the second sheet answers and modify them and resubmit. And then I just grade the last one. If you haven't signed in, make sure you sign in. Otherwise, have a good day."
}